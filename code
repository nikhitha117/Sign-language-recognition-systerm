from tkinter import messagebox
from tkinter import *
from tkinter import simpledialog
import tkinter
from tkinter import filedialog
from tkinter.filedialog import askopenfilename
import cv2
import random
import numpy as np
from keras.utils.np_utils import to_categorical
from keras.layers import MaxPooling2D
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Convolution2D
from keras.models import Sequential
from keras.models import model_from_json
import pickle
import os
import imutils
from gtts import gTTS
from playsound import playsound
from threading import Thread 

main = tkinter.Tk()
main.title("Sign Language Recognition to Text & Voice using CNN Advance")
main.geometry("1300x1200")

global filename
global classifier
bg = None
playcount = 0

# Gesture labels (edit as needed)
names = ['C','Thumbs Down','Fist','I','Ok','Palm','Thumbs up']

def getID(name):
    for i, n in enumerate(names):
        if n == name:
            return i
    return 0  

bgModel = cv2.createBackgroundSubtractorMOG2(0, 50)

def deleteDirectory():
    for f in [x for x in os.listdir('play') if x.endswith(".mp3")]:
        os.remove(os.path.join('play', f))

def play(playcount, gesture):
    class PlayThread(Thread):
        def __init__(self, playcount, gesture):
            Thread.__init__(self) 
            self.gesture = gesture
            self.playcount = playcount
        def run(self):
            t1 = gTTS(text=self.gesture, lang='en', slow=False)
            t1.save(f"play/{self.playcount}.mp3")
            playsound(f"play/{self.playcount}.mp3")
    PlayThread(playcount, gesture).start()            

def remove_background(frame):
    fgmask = bgModel.apply(frame, learningRate=0)
    kernel = np.ones((3, 3), np.uint8)
    fgmask = cv2.erode(fgmask, kernel, iterations=1)
    return cv2.bitwise_and(frame, frame, mask=fgmask)

def uploadDataset():
    global filename
    filename = filedialog.askdirectory(initialdir=".")
    pathlabel.config(text=filename)
    text.delete('1.0', END)
    text.insert(END, f"{filename} loaded\n\n")

def trainCNN():
    global classifier
    text.delete('1.0', END)
    X_train = np.load('model1/X.txt.npy')
    Y_train = np.load('model1/Y.txt.npy')
    text.insert(END, f"CNN is training on total images : {len(X_train)}\n")

    if os.path.exists('model1/model.json'):
        with open('model1/model.json', "r") as json_file:
            classifier = model_from_json(json_file.read())
        classifier.load_weights("model1/model_weights.h5")
        classifier._make_predict_function()   
        with open('model1/history.pckl', 'rb') as f:
            data = pickle.load(f)
        accuracy = data['accuracy'][9] * 100
        text.insert(END, f"CNN Accuracy = {accuracy:.2f}%")
    else:
        classifier = Sequential()
        classifier.add(Convolution2D(32, 3, 3, input_shape=(64, 64, 3), activation='relu'))
        classifier.add(MaxPooling2D(pool_size=(2, 2)))
        classifier.add(Convolution2D(32, 3, 3, activation='relu'))
        classifier.add(MaxPooling2D(pool_size=(2, 2)))
        classifier.add(Flatten())
        classifier.add(Dense(256, activation='relu'))
        classifier.add(Dense(5, activation='softmax'))
        classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
        hist = classifier.fit(X_train, Y_train, batch_size=16, epochs=10, shuffle=True, verbose=2)
        classifier.save_weights('model1/model_weights.h5')            
        with open("model1/model.json", "w") as json_file:
            json_file.write(classifier.to_json())
        with open('model1/history.pckl', 'wb') as f:
            pickle.dump(hist.history, f)
        accuracy = hist.history['accuracy'][-1] * 100
        text.insert(END, f"CNN Accuracy = {accuracy:.2f}%")

def run_avg(image, aWeight):
    global bg
    if bg is None:
        bg = image.copy().astype("float")
    else:
        cv2.accumulateWeighted(image, bg, aWeight)

def segment(image, threshold=25):
    global bg
    diff = cv2.absdiff(bg.astype("uint8"), image)
    thresholded = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)[1]
    cnts, _ = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not cnts:
        return None
    return thresholded, max(cnts, key=cv2.contourArea)

def webcamPredict():
    global playcount
    oldresult = 'none'
    aWeight = 0.5
    camera = cv2.VideoCapture(0)
    top, right, bottom, left = 10, 350, 325, 690
    num_frames = 0
    fgbg2 = cv2.createBackgroundSubtractorKNN()

    while True:
        grabbed, frame = camera.read()
        if not grabbed:
            break
        frame = imutils.resize(frame, width=700)
        frame = cv2.flip(frame, 1)
        clone = frame.copy()
        roi = frame[top:bottom, right:left]
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        gray = cv2.GaussianBlur(gray, (41, 41), 0)

        if num_frames < 30:
            run_avg(gray, aWeight)
        else:
            hand = segment(gray)
            if hand is not None:
                thresholded, segmented = hand
                cv2.drawContours(clone, [segmented + (right, top)], -1, (0, 0, 255))
                roi = fgbg2.apply(roi)
                cv2.imwrite("test.jpg", roi)
                img = cv2.imread("test.jpg")
                img = cv2.resize(img, (64, 64)).reshape(1, 64, 64, 3).astype('float32') / 255
                predict = classifier.predict(img)
                value, cl = np.max(predict), np.argmax(predict)
                result = names[cl]

                if value >= 0.99:
                    cv2.putText(clone, f'Gesture : {result}', (10, 25),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
                    if oldresult != result:
                        play(playcount, result)
                        oldresult = result
                        playcount += 1

        cv2.rectangle(clone, (left, top), (right, bottom), (0,255,0), 2)
        num_frames += 1
        cv2.imshow("Video Feed", clone)
        if cv2.waitKey(1) & 0xFF == ord("q"):
            break

    camera.release()
    cv2.destroyAllWindows()    

# ───── GUI WIDGETS ────────────────────────────────────────────────
font = ('times', 16, 'bold')
title = Label(main, text='Sign Language Recognition to Text & Voice using CNN Advance',
              bg='yellow4', fg='white', font=font, height=3, width=120, anchor=W, justify=CENTER)
title.place(x=0, y=5)

font1 = ('times', 13, 'bold')
upload = Button(main, text="Upload Hand Gesture Dataset", command=uploadDataset, font=font1)
upload.place(x=50, y=100)

pathlabel = Label(main, bg='yellow4', fg='white', font=font1)
pathlabel.place(x=50, y=150)

trainBtn = Button(main, text="Train CNN with Gesture Images", command=trainCNN, font=font1)
trainBtn.place(x=50, y=200)

predictBtn = Button(main, text="Sign Language Recognition from Webcam", command=webcamPredict, font=font1)
predictBtn.place(x=50, y=250)

text = Text(main, height=15, width=78, font=('times', 12, 'bold'))
text.place(x=450, y=100)
scroll = Scrollbar(text); text.configure(yscrollcommand=scroll.set)

deleteDirectory()
main.config(bg='magenta3')
main.mainloop()
